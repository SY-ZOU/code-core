> PointFusion: Deep Sensor Fusion for 3D Bounding Box Estimation
>
> CVPR 2018

## ABSTARCT

PointFusion：**是一种利用2D图像和 3D 点云信息的通用 3D 对象检测方法**。

与使用多级管道或保持传感器和数据集特定假设的现有方法不同，pointfusion在简单且与应用程序无关。 

图像数据和原始点云数据分别由 CNN 和 PointNet 架构独立处理。 然后，结果输出由一个新的**融合网络组合**，该网络使用输入 3D 点作为空间锚点来预测多个 3D 框及其置信度。 

在两个不同的数据集上评估 PointFusion：KITTI 数据集（使用激光雷达相机捕获的驾驶场景）、 SUN-RGBD 数据集（使用 RGB-D 相机捕获室内环境的）。 

> PointFusion 模型在 KITTI 数据集 （左）和 SUN-RGBD  数据集（右）上的示例 3D 对象检测结果。 
>
> <img src="https://superman-cant-fly.gitee.io/pics/2021-11-02.3.07.05.png" />

Pointfusion是第一个无需任何模型调整即可在这些数据集上效果更好的模型。

## INTRODUCTION

3D 对象检测的目标是恢复场景中所有感兴趣对象的 6 DoF 姿态和 3D 边界框尺寸。 

>  6DoF 就是图片上半区6个自由度的示意图（即沿x、y、z三个直角坐标轴方向的移动自由度和绕这三个坐标轴的转动自由度，分别是前/后-上/下-左/右和俯仰(pitch)-偏摆(yaw)-翻滚(roll)共6个自由度），至于 6DoF 的姿态估计就是下半区图所示的对一个或多个物体在空间中进行6个自由度的姿态估计。 
>
> <img src="https://superman-cant-fly.gitee.io/pics/202111020252.png" width=600/> 

MV3D 假设所有对象都可以在点云自上而下的 2D 视图中进行分割，这适用于常见的自动驾驶情况，但不能推广到可以堆叠放置的室内场景。 此外，自顶向下的视图方法往往只适用于诸如汽车，但不适用于其他关键对象类别，例如脚踏车或骑自行车的人。 

与上述方法不同，我们提出的融合架构被设计为与领域无关，并且与 3D 传感器的位置、类型和数量无关。因此，它是通用的，可用于各种机器人应用。

解决将异构图像和 3D 点云数据结合起来的挑战：**将点云输入保留在其原生表示中，并使用异构网络架构处理它们。**

从图像和稀疏点云进行 3D 对象框回归的DNN具有三个主要组成部分：

- 一个现成的 CNN [*Deep residual learning for image recognition*]，它从输入的 RGB 图像作物中提取外观和几何特征。
- PointNet ： 处理原始 3D 点云。
- 融合子网络：它结合了两个输出来预测 3D 边界框。 

这种异构网络架构，充分利用了两个数据源，而没有引入任何数据处理偏差。我们的融合子网络具有新颖的密集 3D 框预测架构，其中对于每个输入 3D 点，网络预测 3D 框相对于该点的角位置。 然后网络使用学习的评分函数来选择最佳预测。 该方法受到空间锚点 [*Faster r-cnn*] 和密集预测 [*Densebox*] 概念的启发。 **直觉是，与直接回归每个角的 3D 位置的架构相比，使用输入 3D 点作为锚点来预测相对空间位置会降低回归目标的方差**。我们证明了密集预测架构的性能优于直接大幅回归 3D 角位置的架构。

**核心思路总结**：输入RGB image **crop**和与之相对应的raw 3D point cloud，分别通过**ResNet**和**PointNet**提取特征，然后对两类特征进行**fusion**并进一步抽象，最后将3D points视为**spatial anchors**并进行**dense prediction**以得到物体的3D bounding box。

> AVOD将RGB和BEV图像经过特征提取后进行fusion，结合了**颜色信息**与**空间分布信息**，但是使用的BEV是经过点云投影得到，存在**空间信息的损失**；F-PointNet使用raw point cloud提取空间几何特征，没有任何信息的损失，但是没有充分利用RGB信息。而PointFusion权衡了二者的利弊，使用raw point cloud的同时辅以颜色信息。

## METHOD

PointFusion 模型从 `2D 图像裁剪`和`由激光雷达传感器生成的3D点云数据`执行 3D 边界框回归。 

当模型与`最先进的 2D 对象检测器`相结合时，例如 [`Faster r-cnn`]，得到了一个完整的 3D 对象检测系统。 

我们将理论上直接的端到端模型留给未来的工作，因为我们已经通过这种更简单的两阶段设置获得了最先进的结果。此外，当前的设置允许我们在不修改融合网络的情况下插入任何最先进的检测器。

PointFusion 具有三个主要组件：

- 一种提取点云特征的 PointNet 网络变体（A）
- 一个提取图像外观特征的CNN（B）
- 一个融合网络：将两者结合起来，为裁剪中的对象放置一个 3D 边界框
  - a vanilla global architecture：一个普通的全局架构（D）
  -  a novel dense fusion network：一种新型的密集融合网络（C）

我们使用`密集的空间锚机制来改进3D盒子预测`，并使用`两个评分函数来选择最佳预测`。

> <img src="https://superman-cant-fly.gitee.io/pics/2021-11-03.4.54.43.png" width=700/>
>
> - PointFusion 有两个特征提取器：
>
> (1)处理原始点云数据的 PointNet 变体 (A) 
>
> (2)和从输入图像中提取视觉特征的 CNN (B)
>
> - 两种融合网络公式：选择得分最高的预测作为最终预测（E）
>
> (1)一种直接回归框角位置 (D) 的普通全局架构
>
> (2)以及一种预测 8 个角中每个角相对于输入点的空间偏移的新型密集架构，如 (C) 所示 ：对于每个输入点，网络预测从角点（红点）到输入点（蓝色）的空间偏移（白色箭头）

### 1. Point Cloud Network

原始的 PointNet 公式不能用于开箱即用的 3D 回归，描述了我们对 PointNet 所做的两个重要更改：

**（1）No BatchNorm**

批归一化在现代神经架构设计中变得不可或缺，**因为它有效地减少了输入特征的协方差偏移**。 

在最初的 PointNet 实现中，所有全连接层后面都跟有一个批归一化层。 然而，我们发现批量归一化阻碍了 3D 边界框估计性能。 批量归一化旨在消除其输入数据中的规模和偏差，但对于 3D 回归任务，**点位置的绝对数值是有帮助的**。 因此，我们的 PointNet 变体移除了所有批量归一化层。

**（2）Input Normalization**

输入归一化，图像边界框上相应 3D 点云是通过查找场景中可以投影到框上的所有点来获得的。 然而，3D 点的空间位置与 2D 框位置高度相关，这会引入不良偏差。  PointNet 应用空间变换器网络 (STN) 来规范化输入空间。 但是，我们发现 STN 无法完全纠正这些偏差。 我们改为使用已知的相机几何结构来计算规范旋转矩阵 R<sub>c</sub>。R<sub>c</sub> 将**`穿过 2D 框中心的线`旋转到相机帧的 z 轴**。

根据相机参数计算**旋转矩阵**以代替PointNet中的T-Net，对RoI对应部分的点云进行归一化处理。

 <img src="https://superman-cant-fly.gitee.io/pics/2021-11-03.5.17.13.png" width=300/>

> 在输入预处理期间，我们计算旋转 Rc 以规范化每个 RoI 内的点云。(region of interest)

### 2.  Fusion Network

融合网络将使用标准 CNN 提取的图像特征和 PointNet 子网络生成的相应点云特征作为输入。 

它的工作是结合这些特征并为目标对象输出一个 3D 边界框。 

**（1）Global fusion network**

如C 所示，全局融合网络处理图像和点云特征，直接回归目标边界框八个角的 3D 位置。 我们对许多融合函数进行了实验，发现将两个向量串联，然后应用多个完全连接的层，可以获得最佳性能。 全局融合网络的损失函数为：

<img src="https://superman-cant-fly.gitee.io/pics/2021-11-03.5.30.44.png" width=300/>

其中 x<sub>i</sub><sup>*</sup> 是真实框角，x<sub>i</sub> 是预测的角位置，L<sub>stn</sub> 是引入的空间变换的正则化损失，以强制学习空间变换矩阵的正交性。

全局融合网络的一个主要缺点是回归目标 x<sub>i</sub><sup>*</sup> 的方差直接取决于特定场景。 对于自动驾驶，系统预计可以检测 1m 到 100m 以上的物体。这种差异给网络带来负担并导致次优性能。 为了解决这个问题，我们转向经过充分研究的 2D 对象检测问题以寻求灵感。一个常见的解决方案不是直接回归 2D 框，而是通过使用滑动窗口或通过预测框相对于空间锚点的位移来生成对象建议。

**（2）Dense fusion network **

在global fusion的基础之上加入了**point-wise feature**(nx64)，处理方式类似PointNet中segmentation部分，经过若干层全连接层后预测每一个点相对box center的**偏移值**以及**得分**，最后选取得分最高的点为预测结果。

该模型背后的主要思想是使用输入 **3D 点作为密集空间锚点**，不是直接回归 3D 框角的绝对位置。预测每个点的相对偏移值，具备更好的**场景扩展性**。可以使得预测和场景的空间范围无关，比如目标物体离你1m或者100m是一样的。

对于每个输入 3D 点，我们预测从该点到附近框角位置的空间偏移。 所以，网络对场景的空间范围变得不可知。 模型架构如C 所示。 我们使用 PointNet 的一种变体，它可以输出逐点特征。 对于每个点，这些都与全局 PointNet 特征和图像特征连接，产生一个 n × 3136 的输入张量。 密集融合网络使用多个层处理此输入，并输出 3D 边界框预测以及每个点的分数。在测试时，选择得分最高的预测作为最终预测。 具体来说，密集融合网络的损失函数为：

<img src="https://superman-cant-fly.gitee.io/pics/2021-11-03.5.40.55.png" width=400/>

其中 N 是输入点的数量，x<sup>i<sub>∗</sub></sup><sub> offset</sub> 是地面真实的框角位置与第 i 个输入点之间的偏移量，x<sup>i</sup><sub> offset</sub>包含预测的偏移量。 L<sub>score</sub> 是得分函数损失，我们将在下一小节深入解释。 

`3D 框参数化`：我们通过8个角对 **3D 框进行参数化**，因为：（1）该表示用于当前最先进的方法 [`Vehicle detection from 3d lidar using fully convolutional network`, ` Multi-view 3d object detection network for autonomous driving`]，这有助于公平比较。  (2) 它概括了具有 N 个参考点的任何 3D 形状，并且它适用于我们的空间锚点方案：我们可以预测空间偏移而不是角的绝对位置。

### 3. Dense Fusion Prediction Scoring

L<sub>score</sub> 函数的目标是，让网络专注于从靠近目标框的点中学习空间偏移。 

我们提出了两个评分函数：（1）一个有监督的评分函数，它直接训练网络来预测一个点是否在目标边界框内；（2）一个无监督的评分函数，让网络选择最佳预测的点。

**（1）Supervised scoring**

使用监督评分损失函数训练网络，来预测一个点是否在目标框内。

让我们将点 i 的偏移回归损失表示为 L<sup>i</sup><sub>offset</sub> ，以及第 i 个点的二元分类损失作为L<sup>i</sup><sub>score</sub>分数。

<img src="https://superman-cant-fly.gitee.io/pics/2021-11-03.5.53.35.png" width=400/>

> 其中 m<sub>i</sub> ∈ {0, 1} 表示第 i 个点是否在目标边界框内。
>
> Lscore 是交叉熵损失：它惩罚点是否在框内的错误预测。 
>
> 根据定义，此监督评分函数将网络学习重点放在 预测点（框内）的空间偏移量上。 
>
> 然而，这种公式可能不会给出最佳结果，因为框内最有把握的点可能不是具有最佳预测的点。

**（2）Unsupervised scoring **

无监督评分的目标是让网络直接学习哪些点可能给出最佳假设，无论它们是否在框内。 

我们需要训练网络为可能产生良好预测的点分配高置信度。 该公式包括两个相互竞争的损失项：我们希望所有点都有高置信度 c<sub>i</sub>。但是，点偏移预测错误的得分与该置信度成正比。 让我们将 L<sup>i</sup><sub>offset</sub> 定义为点 i 的角偏移回归损失(corner offset regression loss)。 那么损失变为：

<img src="https://superman-cant-fly.gitee.io/pics/2021-11-038.01.06.png" width=400/>

> 其中 w 是两项之间的权重因子。 
>
> 第二项编码了增加 ci 置信度的对数奖励。 
>
> 我们凭经验找到最好的 w ，并在我们所有的实验中使用 w = 0.1。

**（3）实验**

实验结果显示无监督方式优于有监督方式，如下表所示，其中ours-dense和ours-final分别采用的是有监督和无监督学习。

<img src="https://superman-cant-fly.gitee.io/pics/2021-11-038.11.25.png" width=600/>





## CONCLUSION

提出了 PointFusion 网络，它可以根据图像和点云信息准确估计 3D 对象边界框。 

我们的模型有两个主要贡献：

- 使用异构网络架构处理输入。 原始点云数据使用 PointNet 模型直接处理，避免了有损输入预处理，如量化或投影。 
- 引入了一种新颖的密集融合网络，它结合了图像和点云表示。 它相对于作为空间锚点的输入 3D 点预测多个 3D 框假设，并自动学习选择最佳假设。

在相同的架构和超参数下，我们的方法能够与在两个截然不同的数据集上保持最好水平。

未来工作的有希望的方向包括将 2D 检测器和 PointFusion 网络组合成一个端到端的 3D 检测器，以及使用时间组件扩展我们的模型，以在视频和点云流中执行联合检测和跟踪。